# AskMistral: Local LLM Chatbot

A privacy-focused chatbot interface that enables interaction with the Mistral-7B large language model through Ollama. This project combines a FastAPI backend with a Streamlit frontend to deliver a complete offline chat experience.

## Features

- Local LLM Integration: Direct interface with Mistral-7B and other Ollama-supported models
- Complete Privacy: All data processing occurs locally with no external API calls or internet required
- Conversation History: Persistent chat history maintained in the sidebar for context reference
- Prompt Suggestions: Pre-defined question templates to guide user interaction
- Automatic Browser Launch: Streamlit interface automatically opens in your default browser
- Process Management: Automatic cleanup of existing processes on application startup

## Technology Stack

- Backend: FastAPI for robust API handling and Ollama integration
- Frontend: Streamlit for responsive and intuitive web interface
- LLM Framework: Ollama for local model management and inference
- Model: Mistral-7B for natural language processing

## Installation & Setup
### 1. Ensure Ollama is installed
```curl -fsSL https://ollama.ai/install.sh | sh``` <br />
```ollama pull mistral```

### 2. Clone and setup the project
```git clone https://github.com/your-username/askMistral.git``` <br />
```cd askMistral``` <br />
```pip install -r requirements.txt```

### 3. Run the application
```python3 run_askmistral.py```

The application will automatically launch in your browser at localhost:3000 (or next available port).

## Performance Notes
- Response times may vary based on hardware capabilities
- GPU acceleration recommended for improved performance
- Initial response may take several seconds as the model loads into memory
